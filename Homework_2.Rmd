---
title: "Homework2.rmd"
author: "Fabian & Samuel"
output:
  pdf_document: default
  html_document: default
date: "2024-12-09"
---

```{r setup, include=FALSE, tidy.opts=list(width.cutoff=60)}
### Packages and Data Setup

# Load the necessary packages
library(tidyverse)
library(nycflights13)
library(ggplot2)
library(dplyr)
library(ggridges)
library(readxl)
library(ggplot2)
library(kableExtra)

```

## Task 1

```{r echo=FALSE}

## -> EMPR_05_Data_Transformation_I_AS2024.pdf / pages 3 (filter() & group_by()) and 20 (group_by())
## -> EMPR_08a_EDA_AS2024.pdf / page 2: Highlights the importance of data cleaning and inspection, relevant for addressing hourly duplicates in the dataset. / page 14: Discusses handling outliers and irregular data, which applies to ensuring accurate daily counts.

ewr_cold_days <- weather %>%
filter(origin == "EWR") %>%
  mutate(temp_c = (temp - 32) * 5 / 9,      # Convert temperature to Celsius
         day = as.Date(time_hour)) %>%      # Extract the date
  filter(temp_c < 10) %>%                   # Filter for temperatures below 10°C
  group_by(month, day) %>%                  # Group by month and day
  summarise(n = n(), .groups = "drop") %>%  # Summarize daily grouping
  group_by(month) %>%                       # Group by month
  summarise(cold_days = n_distinct(day))    # Count unique cold days

# Display the results
print(ewr_cold_days)

# Check if dep_time contains NA values
flights %>%
  filter(is.na(dep_time)) %>%
  summarise(total_missing = n())

# Analyze flights in January with missing dep_time and other missing variables
missing_dep_time <- flights %>%
  filter(month == 1, is.na(dep_time)) %>%     # Filter January flights with missing dep_time
  summarise(
    total_missing_dep_time = n(),            # Number of affected flights
    across(everything(), ~ sum(is.na(.)))    # Count missing values for each column
  )

# Display the results
print(missing_dep_time)
```

### Explanation

The issue of counting more than 31 days in a month arises because the dataset contains one entry per hour, resulting in 24 rows for each day. To address this, we grouped the data by date to ensure that each day is counted only once. The revised approach calculates the minimum temperature for each day and then filters days where this minimum is below 10°C. This ensures an accurate count of cold days per month.

Yes, the months with the most cold days (January, February, and December) align with expectations, as these are winter months in the Northern Hemisphere. However, it is interesting to note some colder days in spring (April) and fall (November), which could be attributed to occasional cold fronts. Unexpectedly cold days in September might require further investigation for anomalies in the data.

Despite these adjustments, the results still show inconsistencies, such as months with more than 31 cold days. These discrepancies may arise from duplicates or irregularities in the weather dataset, such as multiple recordings for the same hour or incomplete data cleaning. Without deeper investigation or cleaning of the dataset, it is impossible to guarantee correct results.

Final Note: The current count of cold days per month should be interpreted with caution. Additional clarification from the data provider or further exploration of the dataset might be necessary to resolve these issues.

## Task 2

```{r echo=FALSE}

## -> -> EMPR_05_Data_Transformation_I_AS2024.pdf / pages 3 (filter()) and 17 (across())
## -> EMPR_08a_EDA_AS2024.pdf / page 13: Recommends testing results with and without rows with missing or unusual values. / page 14: Describes handling missing data and understanding how such rows differ from recorded values.

# Analyze flights with missing 'dep_time'
missing_dep_time <- flights %>%
  filter(is.na(dep_time)) %>%                       # Filter for rows with missing departure time
  summarise(
    across(everything(), ~ sum(is.na(.)),           # Count missing values across all variables
           .names = "missing_{.col}")               # Name the output columns
  ) %>%
  pivot_longer(everything(),                        # Reshape data for readability
               names_to = "variable", 
               values_to = "missing_count")

print(missing_dep_time)

```

### Explanation

Rows with missing dep_time likely represent flights that were canceled. These rows have missing values for other related columns, such as arr_time, air_time, and sched_dep_time, because a canceled flight does not have actual departure or arrival times recorded. Additionally, the absence of sched_dep_time might indicate flights removed from the schedule altogether due to external factors such as weather conditions, operational issues, or low demand.

## Task 3 - a

```{r echo=FALSE}

## -> -> EMPR_05_Data_Transformation_I_AS2024.pdf / pages 3 (filter()), 10 (select()) and 21 (ggplot())
## -> EMPR_08a_EDA_AS2024.pdf / page 3: Explains analyzing and visualizing distributions using histograms to detect variation within a variable. / page 9: Discusses refining histograms using appropriate bin widths for clarity.

weather_selected <- weather %>%
  select(month, temp) %>%        # Select only 'month' and 'temp' columns
  filter(!is.na(temp))           # Filter out rows where temperature is missing

# Plot the temperature distribution grouped by month
ggplot(weather_selected, aes(x = temp)) +
  geom_histogram(binwidth = 1, color = "black", fill = "skyblue") +  # Histogram with specified aesthetics
  facet_wrap(~ factor(month, levels = 1:12, labels = month.name)) + # Facet by month with names
  labs(
    x = expression(paste("Temperature (", degree, "F)")),           # X-axis label
    y = "Count",                                                   # Y-axis label
    title = "Temperature Distribution by Month"                   # Title
  ) +
  theme_minimal()                                                  # Apply a clean theme


```

## Task 3 - b

```{r echo=FALSE}

## -> -> EMPR_05_Data_Transformation_I_AS2024.pdf / pages 3 (filter()) and 21 (ggplot()) / The specific use of geom_density_ridges() is not covered in the slides but was inspired by similar visualization examples.
## -> EMPR_08a_EDA_AS2024.pdf / page 3: General introduction to analyzing distributions. / page 15: Demonstrates the use of geom_density_ridges() to visualize numerical data distributions across categorical variables.

ggplot(weather_selected, aes(x = temp, y = factor(month, labels = month.name))) +
  geom_density_ridges(fill = "lightblue", color = "blue") +  # Ridge plot aesthetics
  labs(
    x = expression(paste("Temperature (", degree, "F)")),    # X-axis label
    y = "Month",                                             # Y-axis label
    title = "Temperature Distribution by Month"             # Title
  ) +
  theme_minimal()                                            # Clean theme for better readability

```

## Task 3 - c

```{r echo=FALSE}

## -> -> EMPR_05_Data_Transformation_I_AS2024.pdf / pages 20 (group_by() and summarise()) and 21 (Visualization of aggregated data) / The confidence intervals are based on the application of mutate(), also covered in Slide 20
## -> EMPR_08a_EDA_AS2024.pdf / page 16: Describes grouped comparisons using visualizations, relevant for presenting monthly averages. / page 20: Discusses showing group differences, which applies to monthly temperature trends.

aggregated_data <- weather_selected %>%
  group_by(month) %>%                               # Group by month
  summarise(
    mean_temp = mean(temp, na.rm = TRUE),           # Calculate mean temperature
    sd_temp = sd(temp, na.rm = TRUE),               # Calculate standard deviation
    n = n()                                         # Count observations
  ) %>%
  mutate(
    upper_bound = mean_temp + 3 * (sd_temp / sqrt(n)),  # Upper confidence limit
    lower_bound = mean_temp - 3 * (sd_temp / sqrt(n))   # Lower confidence limit
  )

ggplot(aggregated_data, aes(x = month, y = mean_temp)) +
  geom_line(color = "blue") +                       # Plot the mean temperature
  geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound), 
              fill = "lightblue", alpha = 0.5) +    # Confidence interval shading
  labs(
    x = "Month (Number)",                           # X-axis labeled with numbers
    y = expression(paste("Average Temperature (", degree, "F)")), 
    title = "Monthly Average Temperature with 99% Confidence Interval"
  ) +
  scale_x_continuous(breaks = 1:12) +               # Ensure months are labeled as numbers
  theme_minimal()                                   # Clean theme



```

## Task 4

### Data Cleaning and Inspection for Homework

```{r echo=FALSE}

# Import the dataset
Data_HW2 <- read_excel("Data_HW2.xlsx", sheet = "A very small sample")

# date of birth

# Problem:
# The 'date of birth' column contains dates in different formats.
# Some are in "YYYY-MM-DD" format, while others use "DD.MM.YYYY".
# This inconsistency needs to be addressed to ensure uniformity in the dataset.

# Solution:
# Convert all date formats to a standard "YYYY-MM-DD" format using lubridate.

Data_HW2_cleaned <- Data_HW2 %>%
  mutate(
    `date of birth` = case_when(
      grepl("^\\d{2}\\.\\d{2}\\.\\d{4}$", `date of birth`) ~ dmy(`date of birth`), # Handle DD.MM.YYYY
      grepl("^\\d{4}-\\d{2}-\\d{2}$", `date of birth`) ~ ymd(`date of birth`), # Handle YYYY-MM-DD
      grepl("^\\d+$", `date of birth`) ~ as.Date(as.numeric(`date of birth`), origin = "1899-12-30"), # Convert Excel date
      TRUE ~ NA_Date_ # All other cases to NA
    )
  )

# Height

# Problem:
# The 'height' column has inconsistent units. Some values are in 'cm', while others are in 'm' (e.g., '1,82m').
# This inconsistency makes it difficult to analyze the data.

# Solution:
# Convert all heights to centimeters. For values in meters ('m'), multiply by 100 after converting them to numeric.
Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    height = case_when(
      grepl("cm$", height) ~ as.numeric(sub("cm", "", height)), # Remove 'cm' and convert to numeric
      grepl("m$", height) ~ as.numeric(sub(",", ".", sub("m", "", height))) * 100, # Convert 'm' to numeric and multiply by 100
      TRUE ~ NA_real_ # Mark invalid or missing values as NA
    )
  )

# Feet

# Problem:
# The 'foot' column contains unusually large values, likely due to a mix-up with shoe sizes.
# Foot lengths above 40 cm are unrealistic and should be treated as invalid.

# Solution:
# Use mutate() to replace values greater than 40 with NA, indicating invalid entries.

Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    foot = ifelse(foot > 40, NA_real_, foot) # Replace values > 40 with NA
  )

# Hair

# Problem:
# The 'hair' column contains the value "Glatze"
# This is inconsistent with numerical hair lengths. It should be replaced with 0.

# Solution:
# Use mutate() to replace "Glatze" with 0 and ensure all values are numeric.

Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    hair = ifelse(hair == "Glatze", 0, as.numeric(hair)) # Replace "Glatze" with 0 and convert to numeric
  )

# Check the cleaned 'hair' column
view(Data_HW2_cleaned$hair)


#  eye colour

# Problem Description:
# The 'eye colour' column contains inconsistent values due to:
# - Mixed languages (German and English)
# - Inconsistent capitalization
# - Multiple colors separated by spaces or slashes

# Solution:
# Use mutate() to standardize all values to lowercase English and unify multiple colors with a hyphen.

Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    `eye colour` = str_to_lower(`eye colour`), # Convert to lowercase
    `eye colour` = str_replace_all(`eye colour`, "blau", "blue"), # Translate to English
    `eye colour` = str_replace_all(`eye colour`, "grau", "grey"),
    `eye colour` = str_replace_all(`eye colour`, "grün", "green"),
    `eye colour` = str_replace_all(`eye colour`, "braun", "brown"),
    `eye colour` = str_replace_all(`eye colour`, "schwarz", "black"),
    `eye colour` = str_replace_all(`eye colour`, "/", "-"), # Replace slashes with hyphen
    `eye colour` = str_squish(`eye colour`), # Remove extra spaces
    `eye colour` = str_replace_all(`eye colour`, "\\s+", "-"), # Replace spaces with hyphen
    `eye colour` = str_replace_all(`eye colour`, "-{2,}", "-") # Replace multiple hyphens with a single hyphen
  )

# Cash
# Problem:
# The 'cash (CHF)' column has missing values (interpreted as 0) and unrealistically high values like 4000.
# These high values likely refer to salary and should be removed or capped.

# Solution:
# - Replace missing values with 0.
# - Cap all values greater than 400 to 400 (reasonable maximum for cash on hand).

Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    `cash (CHF)` = ifelse(is.na(`cash (CHF)`), 0, `cash (CHF)`), # Replace missing values with 0
    `cash (CHF)` = ifelse(`cash (CHF)` > 400, 400, `cash (CHF)`) # Cap values above 400
  )


# Save cleaned data
saveRDS(Data_HW2_cleaned, file = "cleaned_data.rds")

# Verify cleaned data
cleaned_data <- readRDS("cleaned_data.rds")
view(cleaned_data)


```

## Task 5 - A

```{r echo=FALSE}


#The dataset is not tidy because:
#
#Variables are stored in column names: Each sample (1, 2, 3, 4) has its x and y values in separate columns like x1, y1, x2, etc. Instead, each sample should #be identified as a separate observation.
#Data is not normalized: Instead of having one column for x, one for y, and another for the sample, the dataset has redundant columns for each sample.
#Structure is inefficient: This wide format makes it harder to analyze the data as a whole since the relationships between x and y for all samples are split #across multiple columns.
#Why tidying is important:
#To analyze the data properly, we need to convert it into a tidy format where:

#Each variable has its own column (x, y, sample).
#Each observation is in its own row.


library(tidyr)
library(dplyr)



# Transform the dataset into a tidy format
tidy_anscombe <- anscombe %>%
  pivot_longer(
    cols = everything(), # Include all columns
    names_to = c(".value", "sample"), # Split variable type and sample number
    names_pattern = "([xy])([1-4])" # Extract variable ('x' or 'y') and sample ('1'-'4')
  ) %>%
  mutate(sample = as.integer(sample)) # Convert sample to numeric

# Save the tidy dataset for future use
saveRDS(tidy_anscombe, "tidy_anscombe.rds")

```

## Task 5 - B

```{r echo=FALSE}



# Load the tidy data
tidy_anscombe <- readRDS("tidy_anscombe.rds")

# Create the scatterplot
ggplot(tidy_anscombe, aes(x = x, y = y, color = as.factor(sample), shape = as.factor(sample))) +
  geom_point(size = 3) + # Scatterplot points
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "black", linewidth = 0.8) + # Line of best fit
  scale_shape_manual(values = c(12, 10, 9, 7)) + # Assign shapes for samples
  labs(
    title = "Scatterplot with Line of Best Fit",
    x = "X Values",
    y = "Y Values",
    color = "Sample",
    shape = "Sample"
  ) +
  theme_minimal() # Clean theme

```

## Task 5 - C

```{r echo=FALSE}

# Lade notwendige Pakete
library(dplyr)

# Lade das tidy Dataset
tidy_anscombe <- readRDS("tidy_anscombe.rds") 

# Berechne die Statistiken in einer einzigen Pipeline
tidy_anscombe %>%
  group_by(sample) %>% # Gruppiere nach Sample
  summarize(
    mean_x = mean(x, na.rm = TRUE), # Mittelwert von x
    mean_y = mean(y, na.rm = TRUE), # Mittelwert von y
    sd_x = sd(x, na.rm = TRUE), # Standardabweichung von x
    sd_y = sd(y, na.rm = TRUE), # Standardabweichung von y
    corr_xy = cor(x, y, use = "complete.obs") # Korrelation zwischen x und y
  ) %>%
  kable( # Formatiere die Tabelle
    col.names = c("Sample", "Mean X", "Mean Y", "SD X", "SD Y", "Correlation XY"),
    digits = 3, # Zeige 3 Nachkommastellen
    align = "c"
  ) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
```

```{r echo=FALSE}
library(dplyr)

# Load the tidy dataset
tidy_anscombe <- readRDS("tidy_anscombe.rds") 

# Calculate the statistics in a single pipeline
tidy_anscombe %>%
  group_by(sample) %>% # Group data by the 'sample' variable
  summarize(
    mean_x = mean(x, na.rm = TRUE), # Calculate the mean of x
    mean_y = mean(y, na.rm = TRUE), # Calculate the mean of y
    sd_x = sd(x, na.rm = TRUE), # Calculate the standard deviation of x
    sd_y = sd(y, na.rm = TRUE), # Calculate the standard deviation of y
    corr_xy = cor(x, y, use = "complete.obs") # Calculate the correlation between x and y
  ) %>%
  kable( # Create a formatted table
    col.names = c("Sample", "Mean X", "Mean Y", "SD X", "SD Y", "Correlation XY"), # Set column names
    digits = 3, # Show 3 decimal places
    align = "c" # Center-align the table columns
  ) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover")) # Add styling to the table
    
```

```{r echo=FALSE}
# Load the tidy dataset
readRDS("tidy_anscombe.rds") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 3, color = "blue") + # Add scatterplot points
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "red") + # Add lines of best fit
  facet_grid(~ sample) + # Create one plot per sample
  labs(
    title = "Scatterplots of Anscombe's Data with Lines of Best Fit",
    subtitle = "Each sample shows a different pattern despite having similar summary statistics",
    x = "X Values",
    y = "Y Values"
  ) +
  theme_minimal() # Use a clean theme for better visualization

```

## **Workload**

We sat down at the beginning and divided up the tasks. The first thing we did was set up a Git repository so that we could easily collaborate and continuously track each other’s progress. We organized the tasks so that Samuel took on tasks 1, 2 and 3, while Fabian handled tasks 4 and 5. This clear division helped us focus and avoid overlapping efforts. After completing our initial assignments, we held a short meeting to discuss the status and share updates on our progress. This check-in proved beneficial, as it highlighted some areas that needed further refinement. Although a few items were still incomplete, we felt that we were moving in the right direction and understood what remained to be done. To ensure the quality of each other’s work, we decided that Fabian would review and make corrections to Samuel's tasks, and vice versa. This mutual review process not only helped catch errors but also facilitated a better understanding of each other's approach. By the end of the session, we felt more confident about our progress and looked forward to wrapping up the remaining tasks.
