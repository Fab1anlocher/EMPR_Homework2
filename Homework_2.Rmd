---
title: "Homework2.rmd"
author: "Fabian Locher & Samuel Hänni"
date: "2024-12-09"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE, tidy.opts=list(width.cutoff=60)}
### Packages and Data Setup

# Load the necessary packages
library(tidyverse)
library(nycflights13)
library(ggplot2)
library(ggridges)
library(readxl)
library(ggplot2)
library(kableExtra)

```

# Task 1
```{r echo=FALSE}

## -> EMPR_05_Data_Transformation_I_AS2024.pdf / & aggregate())35 &  subset() 24


# Filter for Newark Airport
ewr_weather <- subset(weather, origin == "EWR")

# Convert temperature to Celsius
ewr_weather$temp_c <- (ewr_weather$temp - 32) * 5 / 9

# Extract the date (ignore time)
ewr_weather$date <- as.Date(ewr_weather$time_hour)

# Find the minimum temperature for each day
daily_min_temp <- aggregate(temp_c ~ date, data = ewr_weather, FUN = min, na.rm = TRUE)

# Filter for days with temperatures below 10°C
cold_days <- subset(daily_min_temp, temp_c < 10)

# Count the number of cold days per month
cold_days$month <- format(cold_days$date, "%m")
monthly_cold_days <- table(cold_days$month)

# Display the results
print(monthly_cold_days)

```
### Explanation

Based on the results, the distribution of cold days per month aligns largely with my expectations. New York is known for its cold winters and warm summers, so the high number of cold days in winter months such as January (31 days), February (28 days), and December (28 days) seems reasonable. The data confirms that winter in New York is consistently cold, as expected.

What did surprise me, however, was the number of cold days in March (31 days). I had expected temperatures to begin warming in early spring, but this suggests that New York can experience lingering cold weather even into March. This could reflect colder-than-average weather patterns or specific cold fronts during that month.

In contrast, the results for the summer months show no cold days, reinforcing the idea that New York experiences reliably warm summers. This is consistent with my general understanding of its climate.

Overall, the results match expectations for New York’s climate: cold, steady winters and predictably warm summers. The only notable exception is March, which appears unusually cold. These variations highlight how regional weather patterns can influence a city's climate dynamics, even within familiar seasonal trends.

# Task 2
```{r echo=FALSE}

## -> -> EMPR_05_Data_Transformation_I_AS2024.pdf / pages 3 (filter()) and 17 (across())
## -> EMPR_08a_EDA_AS2024.pdf / page 13: Recommends testing results with and without rows with missing or unusual values. / page 14: Describes handling missing data and understanding how such rows differ from recorded values.

# Analyze flights with missing 'dep_time'
missing_dep_time <- flights %>%
  filter(is.na(dep_time)) %>%                       # Filter for rows with missing departure time
  summarise(
    across(everything(), ~ sum(is.na(.)),           # Count missing values across all variables
           .names = "missing_{.col}")               # Name the output columns
  ) %>%
  pivot_longer(everything(),                        # Reshape data for readability
               names_to = "variable", 
               values_to = "missing_count")

print(missing_dep_time)

```
### Explanation

Rows with missing dep_time likely represent flights that were canceled. These rows have missing values for other related columns, such as arr_time, air_time, and sched_dep_time, because a canceled flight does not have actual departure or arrival times recorded. Additionally, the absence of sched_dep_time might indicate flights removed from the schedule altogether due to external factors such as weather conditions, operational issues, or low demand.

# Task 3a

```{r echo=FALSE}

## -> -> EMPR_05_Data_Transformation_I_AS2024.pdf / pages 3 (filter()), 10 (select()) and 21 (ggplot())
## -> EMPR_08a_EDA_AS2024.pdf / page 3: Explains analyzing and visualizing distributions using histograms to detect variation within a variable. / page 9: Discusses refining histograms using appropriate bin widths for clarity.

weather_selected <- weather %>%
  select(month, temp) %>%        # Select only 'month' and 'temp' columns
  filter(!is.na(temp))           # Filter out rows where temperature is missing

# Plot the temperature distribution grouped by month
ggplot(weather_selected, aes(x = temp)) +
  geom_histogram(binwidth = 1, color = "black", fill = "skyblue") +  # Histogram with specified aesthetics
  facet_wrap(~ factor(month, levels = 1:12, labels = month.name)) + # Facet by month with names
  labs(
    x = expression(paste("Temperature (", degree, "F)")),           # X-axis label
    y = "Count",                                                   # Y-axis label
    title = "Temperature Distribution by Month"                   # Title
  ) +
  theme_minimal()                                                  # Apply a clean theme
```

# Task 3b

```{r echo=FALSE}

## -> -> EMPR_05_Data_Transformation_I_AS2024.pdf / pages 3 (filter()) and 21 (ggplot()) / The specific use of geom_density_ridges() is not covered in the slides but was inspired by similar visualization examples.
## -> EMPR_08a_EDA_AS2024.pdf / page 3: General introduction to analyzing distributions. / page 15: Demonstrates the use of geom_density_ridges() to visualize numerical data distributions across categorical variables.

ggplot(weather_selected, aes(x = temp, y = factor(month, labels = month.name))) +
  geom_density_ridges(fill = "lightblue", color = "blue") +  # Ridge plot aesthetics
  labs(
    x = expression(paste("Temperature (", degree, "F)")),    # X-axis label
    y = "Month",                                             # Y-axis label
    title = "Temperature Distribution by Month"             # Title
  ) +
  theme_minimal()                                            # Clean theme for better readability
```

# Task 3c

```{r echo=FALSE}

## -> -> EMPR_05_Data_Transformation_I_AS2024.pdf / pages 20 (group_by() and summarise()) and 21 (Visualization of aggregated data) / The confidence intervals are based on the application of mutate(), also covered in Slide 20
## -> EMPR_08a_EDA_AS2024.pdf / page 16: Describes grouped comparisons using visualizations, relevant for presenting monthly averages. / page 20: Discusses showing group differences, which applies to monthly temperature trends.

aggregated_data <- weather_selected %>%
  group_by(month) %>%                               # Group by month
  summarise(
    mean_temp = mean(temp, na.rm = TRUE),           # Calculate mean temperature
    sd_temp = sd(temp, na.rm = TRUE),               # Calculate standard deviation
    n = n()                                         # Count observations
  ) %>%
  mutate(
    upper_bound = mean_temp + 3 * (sd_temp / sqrt(n)),  # Upper confidence limit
    lower_bound = mean_temp - 3 * (sd_temp / sqrt(n))   # Lower confidence limit
  )

ggplot(aggregated_data, aes(x = month, y = mean_temp)) +
  geom_line(color = "blue") +                       # Plot the mean temperature
  geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound), 
              fill = "lightblue", alpha = 0.5) +    # Confidence interval shading
  labs(
    x = "Month (Number)",                           # X-axis labeled with numbers
    y = expression(paste("Average Temperature (", degree, "F)")), 
    title = "Monthly Average Temperature with 99% Confidence Interval"
  ) +
  scale_x_continuous(breaks = 1:12) +               # Ensure months are labeled as numbers
  theme_minimal()                                   # Clean theme
```

# Task 4

```{r echo=FALSE, warning=FALSE }
## -> EMPR_06_Import_Export_AS2024.pdf / page 1: read_excel for importing Excel data
## -> EMPR_06_Import_Export_AS2024.pdf / page 10 (bottom): lubridate for working with date formats
## -> EMPR_05_Data_Transformation_I_AS2024.pdf / slides 14-15: mutate for transforming data types

# Import the dataset
Data_HW2 <- read_excel("Data_HW2.xlsx", sheet = "A very small sample")
```

## date of birth

Problem: The 'date of birth' column contains dates in different formats. Some are in "YYYY-MM-DD" format, while others use "DD.MM.YYYY". This inconsistency needs to be addressed to ensure uniformity in the dataset.

Solution: Convert all date formats to a standard "YYYY-MM-DD" format using lubridate.

```{r echo=FALSE, warning=FALSE }
Data_HW2_cleaned <- Data_HW2 %>%
  mutate(
    `date of birth` = case_when(
      grepl("^\\d{2}\\.\\d{2}\\.\\d{4}$", `date of birth`) ~ as.Date(`date of birth`, format = "%d.%m.%Y"),  # DD.MM.YYYY
      grepl("^\\d{4}-\\d{2}-\\d{2}$", `date of birth`) ~ as.Date(`date of birth`),  # YYYY-MM-DD
      `date of birth` == "NA" ~ NA_Date_,  #
      grepl("^\\d+$", `date of birth`) ~ as.Date(as.numeric(`date of birth`), origin = "1899-12-30"),  
      TRUE ~ NA_Date_  # All other cases = na
    )
  )
```

## Height

Problem: The 'height' column has inconsistent units. Some values are in 'cm', while others are in 'm' (example., '1,82m'). This inconsistency makes it difficult to analyze the data.

Solution: Convert all heights to centimeters. For values in meters ('m'), multiply by 100 after converting them to numeric.

```{r echo=FALSE, warning=FALSE }

Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    height = case_when(
      grepl("cm$", height) ~ as.numeric(sub("cm", "", height)), # Remove 'cm' and convert to numeric
      grepl("m$", height) ~ as.numeric(sub(",", ".", sub("m", "", height))) * 100, # Convert 'm' to numeric and multiply by 100
      TRUE ~ NA_real_ # Mark invalid or missing values as NA
    )
  )
```

## Feet

Problem: The 'foot' column contains unusually large values, likely due to a mix-up with shoe sizes. Foot lengths above 40 cm are in our opinion unrealistic and should be treated as invalid.

Solution: Use mutate() to replace values greater than 40 with NA, indicating invalid entries.

```{r echo=FALSE, warning=FALSE }

Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    foot = ifelse(foot > 40, NA_real_, foot) # Replace values > 40 with NA
  )
```

## Hair

Problem: The 'hair' column contains the value "Glatze" This is inconsistent with numerical hair lengths. It should be replaced with 0.

Solution: Use mutate() to replace "Glatze" with 0 and ensure all values are numeric.

```{r echo=FALSE, warning=FALSE }

Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    hair = ifelse(hair == "Glatze", 0, as.numeric(hair)) # Replace "Glatze" with 0 and convert to numeric
  )


```

## eye colour

Problem: The 'eye colour' column contains inconsistent values due to: - Mixed languages (German and English) - Inconsistent capitalization - Multiple colors separated by spaces or slashes

Solution: Use mutate() to standardize all values to lowercase English and unify multiple colors with a hyphen.

```{r echo=FALSE, warning=FALSE }
# Clean column names (replace spaces with dots)
colnames(Data_HW2_cleaned) <- make.names(colnames(Data_HW2_cleaned))

# Transform the data
Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    # Convert all values in the eye.colour column to lowercase
    eye.colour = str_to_lower(eye.colour),
    
    # Translate German colors to English
    eye.colour = str_replace_all(eye.colour, c(
      "blau" = "blue",
      "grau" = "grey",
      "grün" = "green",
      "braun" = "brown",
      "schwarz" = "black"
    )),
    
    # Handle composite colors
    eye.colour = str_replace_all(eye.colour, c(
      "grün-braun" = "green-brown",
      "grün-grey" = "green-grey",
      "blue-grey" = "blue-grey",
      "green-grey" = "green-grey"
    )),
    
    # Clean special characters and spaces
    eye.colour = str_replace_all(eye.colour, "\\/", "-"),  # Replace slashes with hyphens
    eye.colour = str_squish(eye.colour),                  # Remove extra spaces
    eye.colour = str_replace_all(eye.colour, "\\s+", "-"), # Replace spaces with hyphens
    eye.colour = str_replace_all(eye.colour, "-{2,}", "-") # Replace multiple hyphens with one
  )

```
## Cash

Problem: The 'cash (CHF)' column has missing values (interpreted as 0) and unrealistically high values like 4000. These high values likely refer to salary and should be removed or capped.

Solution: - Replace missing values with 0. - Cap all values greater than 400 to 400 (reasonable maximum for cash on hand).

```{r echo=FALSE, warning=FALSE }
# Clean and transform the data
Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    `cash..CHF.` = ifelse(is.na(`cash..CHF.`), 0, `cash..CHF.`), # Replace missing values with 0
    `cash..CHF.` = ifelse(`cash..CHF.` > 400, 400, `cash..CHF.`) # Cap values above 400
  )

# Save the cleaned data
saveRDS(Data_HW2_cleaned, file = "cleaned_data.rds")

# Load and verify the cleaned data
cleaned_data <- readRDS("cleaned_data.rds")

View(cleaned_data)
```

# Task 5a

The dataset is not tidy because: Variables are stored in column names: Each sample (1, 2, 3, 4) has its x and y values in separate columns like x1, y1, x2, etc. Instead, each sample should #be identified as a separate observation. Data is not normalized: Instead of having one column for x, one for y, and another for the sample, the dataset has redundant columns for each sample. Structure is inefficient: This wide format makes it harder to analyze the data as a whole since the relationships between x and y for all samples are split #across multiple columns. Why tidying is important: To analyze the data properly, we need to convert it into a tidy format where:

Each variable has its own column (x, y, sample). Each observation is in its own row.

```{r echo=FALSE}
## -> EMPR_06_Import_Export_AS2024.pdf / page 14: write_rds for loading saved data
## -> EMPR_07_Data_Tidying_AS2024.pdf / slide 4: pivot_longer for tidying data and using regular expressions
## -> EMPR_05_Data_Transformation_I_AS2024.pdf / slides 14-15: mutate for transforming data types



# Transform the dataset into a tidy format
tidy_anscombe <- anscombe %>%
  pivot_longer(
    cols = everything(), # Include all columns
    names_to = c(".value", "sample"), # Split variable type and sample number
    names_pattern = "([xy])([1-4])" # Extract variable ('x' or 'y') and sample ('1'-'4')
  ) %>%
  mutate(sample = as.integer(sample)) # Convert sample to numeric

# Save the tidy dataset for future use
write_rds(tidy_anscombe, "tidy_anscombe.rds")

```

# Task 5b

```{r echo=FALSE}
## -> EMPR_06_Import_Export_AS2024.pdf / page 14: ReadRDS for loading saved data
## -> EMPR_03_Visualization1_AS2024.pdf / page 12: ggplot2 example with scatterplot and trend line, using geom_point and geom_smooth

# Load the tidy data
tidy_anscombe <- readRDS("tidy_anscombe.rds")

# Create the scatterplot
ggplot(tidy_anscombe, aes(x = x, y = y, color = as.factor(sample), shape = as.factor(sample))) +
  geom_point(size = 3) + # Scatterplot points
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "black", linewidth = 0.8) + # Line of best fit
  scale_shape_manual(values = c(12, 10, 9, 7)) + # Assign shapes for samples
  labs(
    title = "Scatterplot with Line of Best Fit",
    x = "X Values",
    y = "Y Values",
    color = "Sample",
    shape = "Sample"
  ) +
  theme_minimal()

```

# Task 5c

```{r echo=FALSE}

## -> EMPR_06_Import_Export_AS2024.pdf / page 14: readRDS for loading saved data
## -> EMPR_05_Data_Transformation_I_AS2024.pdf / page 20: group_by and summarize for data aggregation


# Load the tidy dataset
tidy_anscombe <- readRDS("tidy_anscombe.rds") 

tidy_anscombe %>%
  group_by(sample) %>% # Group  Sample
  summarize(
    mean_x = mean(x, na.rm = TRUE), 
    mean_y = mean(y, na.rm = TRUE), 
    sd_x = sd(x, na.rm = TRUE), 
    sd_y = sd(y, na.rm = TRUE), 
    corr_xy = cor(x, y, use = "complete.obs") # corr between x and y
  ) %>%
  kable( # Formatiere die Tabelle
    col.names = c("Sample", "mean_x", "mean_y", "sd_X", "sd_Y", "corr_xy"),
    digits = 3, # Zeige 3 Nachkommastellen
    align = "c"
  ) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
```

# Task 5d

```{r echo=FALSE}

## -> EMPR_06_Import_Export_AS2024.pdf / page 14: readRDS for loading saved data
## -> EMPR_03_Visualization1_AS2024.pdf / page 12: ggplot2 example with scatterplot and trend line, using geom_point and geom_smooth
## -> EMPR_03_Visualization1_AS2024.pdf / slide 10: facet_grid for creating subplots


# Load the tidy dataset
readRDS("tidy_anscombe.rds") %>%
  ggplot(aes(x = x, y = y, color = as.factor(sample), shape = as.factor(sample))) +
  geom_point(size = 3) + # Add scatterplot points
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "black") + # Add lines of best fit
  facet_grid(~ sample) + # Create one plot per sample
  scale_color_manual(values = c("red", "blue", "green", "purple")) + # Assign colors to each sample
  scale_shape_manual(values = c(12, 10, 9, 7)) + # Assign shapes to each sample
  labs(
    title = "Scatterplots of Anscombe's Data with Lines of Best Fit",
    subtitle = "Each sample shows a different pattern despite having similar summary statistics",
    x = "X Values",
    y = "Y Values",
    color = "Sample",
    shape = "Sample"
  ) +
  theme_minimal() + # Use a clean theme for better visualization
  theme(
    legend.position = "bottom" # Move legend to the bottom for better space usage
  )

```

## **Workload Samuel**

We sat down at the beginning and divided up the tasks. The first thing we did was set up a Git repository so that we could easily collaborate and continuously track each other’s progress. We organized the tasks so that Samuel took on tasks 1, 2 and 3, while Fabian handled tasks 4 and 5. This clear division helped us focus and avoid overlapping efforts. After completing our initial assignments, we held a short meeting to discuss the status and share updates on our progress. This check-in proved beneficial, as it highlighted some areas that needed further refinement. Although a few items were still incomplete, we felt that we were moving in the right direction and understood what remained to be done. To ensure the quality of each other’s work, we decided that Fabian would review and make corrections to Samuel's tasks, and vice versa. This mutual review process not only helped catch errors but also facilitated a better understanding of each other's approach. By the end of the session, we felt more confident about our progress and looked forward to wrapping up the remaining tasks.

## **Workload Fabian**

I began by taking on tasks 4 and 5, focusing on completing them with care. After finishing my assignments, I did a final review of everything to ensure that everything was working as expected. I saw that we had a mistake in number 1, i think we had 35 cold days in january, which is of course rather difficult in a month that only has 31 days. But I think I was able to fix it now, but that also shows how important it is to check the whole thing again
. Once I was satisfied with the quality of the work, I submitted everything. By the end, I felt confident that the tasks were well-executed and met the necessary standards.
