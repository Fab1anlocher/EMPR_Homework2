---
title: "Homework2.rmd"
author: "Fabian & Samuel"
output:
  pdf_document: default
  html_document: default
date: "2024-12-09"
---

```{r setup, include=FALSE, tidy.opts=list(width.cutoff=60)}
### Packages and Data Setup

# Load the necessary packages
library(tidyverse)
library(nycflights13)
library(ggplot2)
library(dplyr)
library(ggridges)
library(readxl)
library(ggplot2)
library(kableExtra)

```

## Task 1

```{r echo=FALSE}
ewr_cold_days <- weather %>%
filter(origin == "EWR") %>%
  mutate(temp_c = (temp - 32) * 5 / 9,      # Convert temperature to Celsius
         day = as.Date(time_hour)) %>%      # Extract the date
  filter(temp_c < 10) %>%                   # Filter for temperatures below 10°C
  group_by(month, day) %>%                  # Group by month and day
  summarise(n = n(), .groups = "drop") %>%  # Summarize daily grouping
  group_by(month) %>%                       # Group by month
  summarise(cold_days = n_distinct(day))    # Count unique cold days

# Display the results
print(ewr_cold_days)

# Check if dep_time contains NA values
flights %>%
  filter(is.na(dep_time)) %>%
  summarise(total_missing = n())

# Analyze flights in January with missing dep_time and other missing variables
missing_dep_time <- flights %>%
  filter(month == 1, is.na(dep_time)) %>%     # Filter January flights with missing dep_time
  summarise(
    total_missing_dep_time = n(),            # Number of affected flights
    across(everything(), ~ sum(is.na(.)))    # Count missing values for each column
  )

# Display the results
print(missing_dep_time)
```

## Task 2

```{r echo=FALSE}

# Pipeline to analyze flights with missing 'dep_time' in January
flights_missing_dep_time <- flights %>%
  filter(month == 1, is.na(dep_time)) %>%
  summarise(
    total_missing_dep_time = n(),                # Number of flights without departure time
    missing_sched_dep_time = sum(is.na(sched_dep_time)),  # Number of missing 'sched_dep_time' in these flights
    missing_arr_time = sum(is.na(arr_time)),      # Number of missing 'arr_time' in these flights
    missing_air_time = sum(is.na(air_time))       # Number of missing 'air_time' in these flights
  )

# Display the results
print(flights_missing_dep_time)

```

## Task 3 - a

```{r echo=FALSE}

# Example for creating 'weather_selected'
weather_selected <- weather %>%
  select(month, temp) %>%  # Select only the variables 'month' and 'temp'
  filter(!is.na(temp))     # Filter out all rows with missing 'temp' values

# a) Distribution of temperatures grouped by months with month names on the y-axis
ggplot(weather_selected, aes(x = temp)) +
  geom_histogram(binwidth = 1, color = "black", fill = "skyblue") +
  facet_wrap(~ factor(month, levels = 1:12, labels = month.name), scales = "free_y") +  # Use month.name for labels
  labs(x = expression(paste("Temperature (", degree, "F)")), y = "Count", title = "Temperature Distribution by Month") +  # Use expression()
  scale_x_continuous(breaks = seq(-20, 100, 10)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_minimal() +
  theme(strip.text = element_text(size = 12),
        axis.text = element_text(size = 10))

```

## Task 3 - b

```{r echo=FALSE}
# b) Density plot using ggridges::geom_density_ridges()
ggplot(weather_selected, aes(x = temp, y = factor(month))) +
  geom_density_ridges(scale = 3, fill = "lightblue", color = "blue") +  # Create ridge plots with specified colors and scaling
  labs(x = expression(paste("Temperature (", degree, "F)")), y = "Month", title = "Density Ridge Plot of Temperature by Month") +  # Add labels and title
  scale_y_discrete(labels = month.name) +  # Use month.name to label the y-axis
  theme_minimal()  # Apply a clean minimal theme

```

## Task 3 - c

```{r echo=FALSE}
# c) Aggregated monthly average temperature and standard deviation
aggregated_data <- weather_selected %>%
  group_by(month) %>%  # Group data by month
  summarise(
    mean_temp = mean(temp, na.rm = TRUE),  # Calculate the mean temperature for each month
    sd_temp = sd(temp, na.rm = TRUE),      # Calculate the standard deviation of the temperature
    n = n()                                # Count the number of observations per month
  ) %>%
  mutate(
    upper_bound = mean_temp + 3 * (sd_temp / sqrt(n)),  # Calculate the upper bound for the 99% confidence interval
    lower_bound = mean_temp - 3 * (sd_temp / sqrt(n))   # Calculate the lower bound for the 99% confidence interval
  )

# Plot for monthly average temperature with confidence intervals
ggplot(aggregated_data, aes(x = month, y = mean_temp)) +
  geom_line(color = "blue", size = 1) +  # Line plot for average temperature
  geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound), fill = "lightblue", alpha = 0.5) +  # Confidence interval ribbon
  labs(
    x = "Month", 
    y = expression(paste("Average Temperature (", degree, "F)")), 
    title = "Monthly Average Temperature with 99% Confidence Interval"
  ) +
  scale_x_continuous(breaks = 1:12, labels = month.name) +  # Label months with their names
  theme_minimal() +  # Apply a clean minimal theme
  theme(axis.text = element_text(size = 10))  # Customize axis text size

```

## Task 4

# Data Cleaning and Inspection for Homework

```{r echo=FALSE}

# Import the dataset
Data_HW2 <- read_excel("Data_HW2.xlsx", sheet = "A very small sample")

# date of birth

# Problem:
# The 'date of birth' column contains dates in different formats.
# Some are in "YYYY-MM-DD" format, while others use "DD.MM.YYYY".
# This inconsistency needs to be addressed to ensure uniformity in the dataset.

# Solution:
# Convert all date formats to a standard "YYYY-MM-DD" format using lubridate.

Data_HW2_cleaned <- Data_HW2 %>%
  mutate(
    `date of birth` = case_when(
      grepl("^\\d{2}\\.\\d{2}\\.\\d{4}$", `date of birth`) ~ dmy(`date of birth`), # Handle DD.MM.YYYY
      grepl("^\\d{4}-\\d{2}-\\d{2}$", `date of birth`) ~ ymd(`date of birth`), # Handle YYYY-MM-DD
      grepl("^\\d+$", `date of birth`) ~ as.Date(as.numeric(`date of birth`), origin = "1899-12-30"), # Convert Excel date
      TRUE ~ NA_Date_ # All other cases to NA
    )
  )

# Height

# Problem:
# The 'height' column has inconsistent units. Some values are in 'cm', while others are in 'm' (e.g., '1,82m').
# This inconsistency makes it difficult to analyze the data.

# Solution:
# Convert all heights to centimeters. For values in meters ('m'), multiply by 100 after converting them to numeric.
Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    height = case_when(
      grepl("cm$", height) ~ as.numeric(sub("cm", "", height)), # Remove 'cm' and convert to numeric
      grepl("m$", height) ~ as.numeric(sub(",", ".", sub("m", "", height))) * 100, # Convert 'm' to numeric and multiply by 100
      TRUE ~ NA_real_ # Mark invalid or missing values as NA
    )
  )

# Feet

# Problem:
# The 'foot' column contains unusually large values, likely due to a mix-up with shoe sizes.
# Foot lengths above 40 cm are unrealistic and should be treated as invalid.

# Solution:
# Use mutate() to replace values greater than 40 with NA, indicating invalid entries.

Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    foot = ifelse(foot > 40, NA_real_, foot) # Replace values > 40 with NA
  )

# Hair

# Problem:
# The 'hair' column contains the value "Glatze"
# This is inconsistent with numerical hair lengths. It should be replaced with 0.

# Solution:
# Use mutate() to replace "Glatze" with 0 and ensure all values are numeric.

Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    hair = ifelse(hair == "Glatze", 0, as.numeric(hair)) # Replace "Glatze" with 0 and convert to numeric
  )

# Check the cleaned 'hair' column
view(Data_HW2_cleaned$hair)


#  eye colour

# Problem Description:
# The 'eye colour' column contains inconsistent values due to:
# - Mixed languages (German and English)
# - Inconsistent capitalization
# - Multiple colors separated by spaces or slashes

# Solution:
# Use mutate() to standardize all values to lowercase English and unify multiple colors with a hyphen.

Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    `eye colour` = str_to_lower(`eye colour`), # Convert to lowercase
    `eye colour` = str_replace_all(`eye colour`, "blau", "blue"), # Translate to English
    `eye colour` = str_replace_all(`eye colour`, "grau", "grey"),
    `eye colour` = str_replace_all(`eye colour`, "grün", "green"),
    `eye colour` = str_replace_all(`eye colour`, "braun", "brown"),
    `eye colour` = str_replace_all(`eye colour`, "schwarz", "black"),
    `eye colour` = str_replace_all(`eye colour`, "/", "-"), # Replace slashes with hyphen
    `eye colour` = str_squish(`eye colour`), # Remove extra spaces
    `eye colour` = str_replace_all(`eye colour`, "\\s+", "-"), # Replace spaces with hyphen
    `eye colour` = str_replace_all(`eye colour`, "-{2,}", "-") # Replace multiple hyphens with a single hyphen
  )

# Cash
# Problem:
# The 'cash (CHF)' column has missing values (interpreted as 0) and unrealistically high values like 4000.
# These high values likely refer to salary and should be removed or capped.

# Solution:
# - Replace missing values with 0.
# - Cap all values greater than 400 to 400 (reasonable maximum for cash on hand).

Data_HW2_cleaned <- Data_HW2_cleaned %>%
  mutate(
    `cash (CHF)` = ifelse(is.na(`cash (CHF)`), 0, `cash (CHF)`), # Replace missing values with 0
    `cash (CHF)` = ifelse(`cash (CHF)` > 400, 400, `cash (CHF)`) # Cap values above 400
  )


# Save cleaned data
saveRDS(Data_HW2_cleaned, file = "cleaned_data.rds")

# Verify cleaned data
cleaned_data <- readRDS("cleaned_data.rds")
view(cleaned_data)


```

## Task 5 - A

```{r echo=FALSE}


#The dataset is not tidy because:
#
#Variables are stored in column names: Each sample (1, 2, 3, 4) has its x and y values in separate columns like x1, y1, x2, etc. Instead, each sample should #be identified as a separate observation.
#Data is not normalized: Instead of having one column for x, one for y, and another for the sample, the dataset has redundant columns for each sample.
#Structure is inefficient: This wide format makes it harder to analyze the data as a whole since the relationships between x and y for all samples are split #across multiple columns.
#Why tidying is important:
#To analyze the data properly, we need to convert it into a tidy format where:

#Each variable has its own column (x, y, sample).
#Each observation is in its own row.


library(tidyr)
library(dplyr)



# Transform the dataset into a tidy format
tidy_anscombe <- anscombe %>%
  pivot_longer(
    cols = everything(), # Include all columns
    names_to = c(".value", "sample"), # Split variable type and sample number
    names_pattern = "([xy])([1-4])" # Extract variable ('x' or 'y') and sample ('1'-'4')
  ) %>%
  mutate(sample = as.integer(sample)) # Convert sample to numeric

# Save the tidy dataset for future use
saveRDS(tidy_anscombe, "tidy_anscombe.rds")

```

# Task 5 - B

```{r echo=FALSE}



# Load the tidy data
tidy_anscombe <- readRDS("tidy_anscombe.rds")

# Create the scatterplot
ggplot(tidy_anscombe, aes(x = x, y = y, color = as.factor(sample), shape = as.factor(sample))) +
  geom_point(size = 3) + # Scatterplot points
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "black", linewidth = 0.8) + # Line of best fit
  scale_shape_manual(values = c(12, 10, 9, 7)) + # Assign shapes for samples
  labs(
    title = "Scatterplot with Line of Best Fit",
    x = "X Values",
    y = "Y Values",
    color = "Sample",
    shape = "Sample"
  ) +
  theme_minimal() # Clean theme

```

# Task 5 - C

```{r echo=FALSE}

# Lade notwendige Pakete
library(dplyr)

# Lade das tidy Dataset
tidy_anscombe <- readRDS("tidy_anscombe.rds") 

# Berechne die Statistiken in einer einzigen Pipeline
tidy_anscombe %>%
  group_by(sample) %>% # Gruppiere nach Sample
  summarize(
    mean_x = mean(x, na.rm = TRUE), # Mittelwert von x
    mean_y = mean(y, na.rm = TRUE), # Mittelwert von y
    sd_x = sd(x, na.rm = TRUE), # Standardabweichung von x
    sd_y = sd(y, na.rm = TRUE), # Standardabweichung von y
    corr_xy = cor(x, y, use = "complete.obs") # Korrelation zwischen x und y
  ) %>%
  kable( # Formatiere die Tabelle
    col.names = c("Sample", "Mean X", "Mean Y", "SD X", "SD Y", "Correlation XY"),
    digits = 3, # Zeige 3 Nachkommastellen
    align = "c"
  ) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))
```
    
    
```{r echo=FALSE}    
library(dplyr)

# Load the tidy dataset
tidy_anscombe <- readRDS("tidy_anscombe.rds") 

# Calculate the statistics in a single pipeline
tidy_anscombe %>%
  group_by(sample) %>% # Group data by the 'sample' variable
  summarize(
    mean_x = mean(x, na.rm = TRUE), # Calculate the mean of x
    mean_y = mean(y, na.rm = TRUE), # Calculate the mean of y
    sd_x = sd(x, na.rm = TRUE), # Calculate the standard deviation of x
    sd_y = sd(y, na.rm = TRUE), # Calculate the standard deviation of y
    corr_xy = cor(x, y, use = "complete.obs") # Calculate the correlation between x and y
  ) %>%
  kable( # Create a formatted table
    col.names = c("Sample", "Mean X", "Mean Y", "SD X", "SD Y", "Correlation XY"), # Set column names
    digits = 3, # Show 3 decimal places
    align = "c" # Center-align the table columns
  ) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover")) # Add styling to the table
    
 ```   
  
```{r echo=FALSE}
# Load the tidy dataset
readRDS("tidy_anscombe.rds") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 3, color = "blue") + # Add scatterplot points
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "red") + # Add lines of best fit
  facet_grid(~ sample) + # Create one plot per sample
  labs(
    title = "Scatterplots of Anscombe's Data with Lines of Best Fit",
    subtitle = "Each sample shows a different pattern despite having similar summary statistics",
    x = "X Values",
    y = "Y Values"
  ) +
  theme_minimal() # Use a clean theme for better visualization

```



### **Workload**

### We sat down at the beginning and divided up the tasks. The first thing we did was set up a Git repository so that we could easily collaborate and continuously track each other’s progress. We organized the tasks so that Samuel took on tasks 1, 2 and 3, while Fabian handled tasks 4 and 5. This clear division helped us focus and avoid overlapping efforts. After completing our initial assignments, we held a short meeting to discuss the status and share updates on our progress. This check-in proved beneficial, as it highlighted some areas that needed further refinement. Although a few items were still incomplete, we felt that we were moving in the right direction and understood what remained to be done. To ensure the quality of each other’s work, we decided that Fabian would review and make corrections to Samuel's tasks, and vice versa. This mutual review process not only helped catch errors but also facilitated a better understanding of each other's approach. By the end of the session, we felt more confident about our progress and looked forward to wrapping up the remaining tasks.
